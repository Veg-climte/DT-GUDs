{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0856fa21-6ec4-4936-8ef5-1bb13d530ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "XGBoost + RFE (backward) + forward selection, with SHAP reporting.\n",
    "Steps (English):\n",
    "1) Load dependent raster and explanatory rasters, align, mask to valid pixels.\n",
    "2) Robustly clip each explanatory raster to [1st, 99th] percentiles.\n",
    "3) Train an initial XGBoost model; report baseline R^2/RMSE.\n",
    "4) RFE (backward elimination): iteratively remove the least-important feature\n",
    "   until FINAL_FEATURE_COUNT; record a FULL importance ranking.\n",
    "5) Forward selection: start from top-k, add features only if ΔR^2 >= R2_THRESHOLD.\n",
    "6) Train final model on the selected set, save metrics & SHAP-based importance.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel, delayed\n",
    "import shap\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Config\n",
    "# ---------------------------\n",
    "NUM_CPUS = 32                 # parallel raster loading\n",
    "R2_THRESHOLD = 0.01          # min ΔR^2 to keep a feature in forward selection\n",
    "FINAL_FEATURE_COUNT = 3       # target size for RFE and initial seed for forward selection\n",
    "TEST_SIZE = 0.30\n",
    "RANDOM_STATE = 42\n",
    "PERC_LOW, PERC_HIGH = 1, 99   # robust clipping\n",
    "MIN_VALID_SAMPLES = 100       # avoid training on tiny samples\n",
    "\n",
    "# ---------------------------\n",
    "# Utilities\n",
    "# ---------------------------\n",
    "def robust_percentiles(arr, low=1, high=99):\n",
    "    \"\"\"Return (lo, hi) on finite values only; None if degenerate.\"\"\"\n",
    "    vals = np.asarray(arr)\n",
    "    finite = np.isfinite(vals)\n",
    "    if not finite.any():\n",
    "        return None\n",
    "    lo = np.nanpercentile(vals[finite], low)\n",
    "    hi = np.nanpercentile(vals[finite], high)\n",
    "    if not (np.isfinite(lo) and np.isfinite(hi) and lo < hi):\n",
    "        return None\n",
    "    return float(lo), float(hi)\n",
    "\n",
    "def load_and_process_data(file, dep_data):\n",
    "    \"\"\"Read, align, mask to dep valid pixels, robustly clip to [1,99] percentiles.\"\"\"\n",
    "    da = rxr.open_rasterio(file).squeeze().astype(\"float32\")\n",
    "    da = da.rio.reproject_match(dep_data)\n",
    "    da = da.where(~dep_data.isnull())\n",
    "    bounds = robust_percentiles(da.values, PERC_LOW, PERC_HIGH)\n",
    "    if bounds is not None:\n",
    "        lo, hi = bounds\n",
    "        da = da.clip(min=lo, max=hi)\n",
    "    return da\n",
    "\n",
    "def prepare_dataset(explanatory_files, dep_data, n_jobs=NUM_CPUS):\n",
    "    \"\"\"Return y (dep) and X (stacked features) as 2D arrays with valid rows only.\"\"\"\n",
    "    exps = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(load_and_process_data)(f, dep_data) for f in explanatory_files\n",
    "    )\n",
    "    y = dep_data.values.flatten().astype(\"float32\")\n",
    "    X = np.vstack([e.values.flatten().astype(\"float32\") for e in exps]).T\n",
    "\n",
    "    mask = (\n",
    "        np.isfinite(y) & np.isfinite(X).all(axis=1)\n",
    "    )\n",
    "    y, X = y[mask], X[mask]\n",
    "    return y, X\n",
    "\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Fit XGBoost and return (model, R2, RMSE).\"\"\"\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=None,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=1,   # avoid conflict with joblib\n",
    "        tree_method=\"hist\"  # fast & stable default\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    return model, float(r2), rmse\n",
    "\n",
    "# ---------------------------\n",
    "# Feature selection\n",
    "# ---------------------------\n",
    "def recursive_feature_elimination(explanatory_files, dep_data):\n",
    "    \"\"\"\n",
    "    Backward elimination down to FINAL_FEATURE_COUNT.\n",
    "    Returns:\n",
    "      survivors: list[str]  (size == FINAL_FEATURE_COUNT)\n",
    "      full_ranking: list[str]  (most->least important overall)\n",
    "    \"\"\"\n",
    "    remaining = explanatory_files.copy()\n",
    "    removed_order = []  # tuples of (feature, importance) in removal order (least important at each step)\n",
    "\n",
    "    print(\"\\n[RFE] Start backward elimination …\")\n",
    "    while len(remaining) > FINAL_FEATURE_COUNT:\n",
    "        y, X = prepare_dataset(remaining, dep_data)\n",
    "        if y.size < MIN_VALID_SAMPLES:\n",
    "            print(\"[RFE] Too few valid samples; aborting RFE.\")\n",
    "            break\n",
    "\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "        model, r2, rmse = train_and_evaluate(X_tr, X_te, y_tr, y_te)\n",
    "        importances = model.feature_importances_\n",
    "\n",
    "        # Identify least important feature in the current set\n",
    "        least_idx = int(np.argmin(importances))\n",
    "        least_feat = remaining[least_idx]\n",
    "        least_imp = float(importances[least_idx])\n",
    "        print(f\"[RFE] Remove: {os.path.basename(least_feat)} (importance={least_imp:.6f})  | R²={r2:.4f}, RMSE={rmse:.4f}\")\n",
    "\n",
    "        removed_order.append((least_feat, least_imp))\n",
    "        remaining.pop(least_idx)\n",
    "\n",
    "    # Build a full ranking:\n",
    "    # - Features removed earlier had the lowest importance → put them at the bottom.\n",
    "    # - Survivors are considered the most important; sort survivors by a final fit to get an order.\n",
    "    if len(remaining) > 1:\n",
    "        y, X = prepare_dataset(remaining, dep_data)\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "        model, _, _ = train_and_evaluate(X_tr, X_te, y_tr, y_te)\n",
    "        surv_imps = model.feature_importances_\n",
    "        survivors_sorted = [f for _, f in sorted(zip(-surv_imps, remaining))]\n",
    "    else:\n",
    "        survivors_sorted = remaining.copy()\n",
    "\n",
    "    removed_features_in_order = [f for f, _ in removed_order]  # from first removed → last removed\n",
    "    # Most->least important: survivors (sorted by importance) followed by removed (reverse removal order)\n",
    "    full_ranking = survivors_sorted + removed_features_in_order[::-1]\n",
    "    print(f\"[RFE] Done. Survivors: {[os.path.basename(f) for f in survivors_sorted]}\")\n",
    "    return survivors_sorted, full_ranking\n",
    "\n",
    "def forward_feature_selection(feature_ranking, dep_data):\n",
    "    \"\"\"\n",
    "    Forward selection starting from the top-k (k=FINAL_FEATURE_COUNT) seed.\n",
    "    Adds features only if ΔR² >= R2_THRESHOLD.\n",
    "    Returns (selected_features, best_r2).\n",
    "    \"\"\"\n",
    "    if len(feature_ranking) <= FINAL_FEATURE_COUNT:\n",
    "        # nothing to add\n",
    "        seed = feature_ranking.copy()\n",
    "        y, X = prepare_dataset(seed, dep_data)\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "        _, r2, _ = train_and_evaluate(X_tr, X_te, y_tr, y_te)\n",
    "        return seed, r2\n",
    "\n",
    "    selected = feature_ranking[:FINAL_FEATURE_COUNT].copy()\n",
    "    remaining = feature_ranking[FINAL_FEATURE_COUNT:].copy()\n",
    "\n",
    "    # Evaluate the seed set first\n",
    "    y, X = prepare_dataset(selected, dep_data)\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "    _, best_r2, _ = train_and_evaluate(X_tr, X_te, y_tr, y_te)\n",
    "    print(f\"\\n[Forward] Seed (top-{FINAL_FEATURE_COUNT}) R² = {best_r2:.4f}\")\n",
    "\n",
    "    while remaining:\n",
    "        cand = remaining.pop(0)\n",
    "        trial = selected + [cand]\n",
    "\n",
    "        y, X = prepare_dataset(trial, dep_data)\n",
    "        X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "        _, r2, _ = train_and_evaluate(X_tr, X_te, y_tr, y_te)\n",
    "        delta = r2 - best_r2\n",
    "        print(f\"[Forward] Try +{os.path.basename(cand)}: R²={r2:.4f} (Δ={delta:.4f})\")\n",
    "\n",
    "        if delta >= R2_THRESHOLD:\n",
    "            selected.append(cand)\n",
    "            best_r2 = r2\n",
    "            print(f\"[Forward] Kept: {os.path.basename(cand)}  | New best R²={best_r2:.4f}\")\n",
    "        else:\n",
    "            print(f\"[Forward] Discarded: {os.path.basename(cand)} (Δ<{R2_THRESHOLD})\")\n",
    "\n",
    "    return selected, best_r2\n",
    "\n",
    "# ---------------------------\n",
    "# Main\n",
    "# ---------------------------\n",
    "def main():\n",
    "    # --- Load dependent variable (edit path) ---\n",
    "    dep_file = r\"I:\\path\\to\\dependent.tif\"\n",
    "    dep_data = rxr.open_rasterio(dep_file).squeeze()\n",
    "    dep_data = dep_data.where((dep_data > -10) & (dep_data <= 0))  # example mask for [-10, 0]\n",
    "\n",
    "    # --- Initial explanatory rasters (edit list) ---\n",
    "    explanatory_files = [\n",
    "        r\"I:\\path\\to\\ai_v3_yra_.tif\",\n",
    "        r\"I:\\path\\to\\KNDVI_alpha.tif\",\n",
    "        r\"I:\\path\\to\\Srad.tif\",\n",
    "        r\"I:\\path\\to\\VPD.tif\",\n",
    "        r\"I:\\path\\to\\Wind.tif\",\n",
    "        r\"I:\\path\\to\\Annual_ppt_1982_2021.tif\",\n",
    "        r\"I:\\path\\to\\Vegetation_species.tif\",\n",
    "        r\"I:\\path\\to\\wc2.1_2.5m_elev.tif\",\n",
    "        r\"I:\\path\\to\\mean_NDVI_.tif\",\n",
    "        r\"I:\\path\\to\\Tmax_Pfreq.tif\",\n",
    "        r\"I:\\path\\to\\SPEI_1982_2021.tif\",\n",
    "        r\"I:\\path\\to\\NDVI_EOS_POS_Difference_LAI.tif\",\n",
    "        r\"I:\\path\\to\\rplant_proxy.tif\",\n",
    "    ]\n",
    "\n",
    "    # --- Initial model ---\n",
    "    y, X = prepare_dataset(explanatory_files, dep_data)\n",
    "    if y.size < MIN_VALID_SAMPLES:\n",
    "        raise RuntimeError(\"Too few valid samples after masking/clipping. Check rasters & alignment.\")\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "    init_model, init_r2, init_rmse = train_and_evaluate(X_tr, X_te, y_tr, y_te)\n",
    "    print(f\"\\n[Init] R²={init_r2:.4f}, RMSE={init_rmse:.4f}  (n={y.size})\")\n",
    "\n",
    "    # Save initial predictions\n",
    "    with open(\"initial_xgb_results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(\n",
    "            {\"y_test\": y_te, \"y_pred\": init_model.predict(X_te), \"r2\": init_r2, \"rmse\": init_rmse},\n",
    "            f\n",
    "        )\n",
    "\n",
    "    # --- RFE to FINAL_FEATURE_COUNT, plus full ranking ---\n",
    "    survivors, full_ranking = recursive_feature_elimination(explanatory_files, dep_data)\n",
    "\n",
    "    # If you prefer to forward-select using full_ranking (most->least important):\n",
    "    selected_features, forward_r2 = forward_feature_selection(full_ranking, dep_data)\n",
    "\n",
    "    # --- Final model on selected features ---\n",
    "    yF, XF = prepare_dataset(selected_features, dep_data)\n",
    "    X_trF, X_teF, y_trF, y_teF = train_test_split(XF, yF, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "    final_model, final_r2, final_rmse = train_and_evaluate(X_trF, X_teF, y_trF, y_teF)\n",
    "    print(f\"\\n[Final] Features={len(selected_features)}  R²={final_r2:.4f}, RMSE={final_rmse:.4f}\")\n",
    "\n",
    "    with open(\"final_xgb_results.pkl\", \"wb\") as f:\n",
    "        pickle.dump(\n",
    "            {\"y_test\": y_teF, \"y_pred\": final_model.predict(X_teF), \"r2\": final_r2, \"rmse\": final_rmse},\n",
    "            f\n",
    "        )\n",
    "\n",
    "    # --- SHAP on final model (test set) ---\n",
    "    # For tree models in SHAP v0.41+, you can use: shap.Explainer(final_model)\n",
    "    explainer = shap.TreeExplainer(final_model)\n",
    "    shap_values = explainer.shap_values(X_teF)  # array (n_test, n_features)\n",
    "\n",
    "    shap_mean_abs = np.abs(shap_values).mean(axis=0)\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"Feature\": [os.path.basename(f) for f in selected_features],\n",
    "        \"Mean_ABS_SHAP\": shap_mean_abs\n",
    "    }).sort_values(by=\"Mean_ABS_SHAP\", ascending=False)\n",
    "\n",
    "    shap_df.to_csv(\"final_shap_feature_importances.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(shap_df[\"Feature\"], shap_df[\"Mean_ABS_SHAP\"])\n",
    "    plt.xlabel(\"Mean |SHAP| (importance)\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.title(\"SHAP Feature Importance (Final Model)\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"final_shap_feature_importances.png\", dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
